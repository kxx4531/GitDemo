{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDdAqyBffEWz",
        "outputId": "42a21455-2ea7-4696-9d72-d670ad35d984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 续写结果 ===\n",
            "如 果 动 物 会 说 话 ， 它 们 最 想 告 诉 人 类 的 是 第 一 个 问 题 吗 ？ 这 个 问 题 的 答 案 应 该 不 是 吧 。 人 在 面 对 生 存 和 死 亡 时 ， 想 知 道 是 什 么 问 题 或 者 理 由 。 你 是 怎 样 看 待 动 物 这 件 事 情 的 。 我 想 回 答 关 于 这 个 问 题 的 问 题 。 - - - - - - - -\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 根据学号选取的句子开头\n",
        "prompt = \"如果动物会说话，它们最想告诉人类的是\"\n",
        "\n",
        "\n",
        "# 加载预训练模型和分词器\n",
        "model_name = \"uer/gpt2-chinese-cluecorpussmall\"  # Hugging Face上的中文GPT模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 将模型设置为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 文本生成配置\n",
        "generation_config = {\n",
        "    \"max_length\": 100,\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 0.9,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"do_sample\": True,\n",
        "    \"num_return_sequences\": 1\n",
        "}\n",
        "\n",
        "# 生成续写文本\n",
        "def generate_continuation(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            **generation_config\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 执行生成\n",
        "generated_text = generate_continuation(prompt)\n",
        "\n",
        "# 输出结果\n",
        "print(\"=== 续写结果 ===\")\n",
        "print(generated_text)"
      ]
    }
  ]
}